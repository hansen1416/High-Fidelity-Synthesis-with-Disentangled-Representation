{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"idgan.ipynb","provenance":[],"collapsed_sections":["-X4PQKZ-rqO-","1jZVKqG8r0no"],"machine_shape":"hm","authorship_tag":"ABX9TyMjXZsqVNG6z+enfqgOKKC4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"3da2f321e129438aa361fc61e76d6c58":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ab19164b0436479aac261098175892f3","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c3dad5276dce48edb5a0584a4c828c6c","IPY_MODEL_faefd5b758a24f4da72799a91babfaaf"]}},"ab19164b0436479aac261098175892f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c3dad5276dce48edb5a0584a4c828c6c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5fcdf5f39e574e8ea89e700d5fc1a8c2","_dom_classes":[],"description":"[500000] recon_loss:217.301 kld:16.852: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":500000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":500000,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_68113d15a7d44ae297a0e3e448fb4e50"}},"faefd5b758a24f4da72799a91babfaaf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d763f0fca48e432ea91b0a348276c5af","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 500000/500000 [6:56:04&lt;00:00, 20.03it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_28e09dcdfa664e419af395c7de153d26"}},"5fcdf5f39e574e8ea89e700d5fc1a8c2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"68113d15a7d44ae297a0e3e448fb4e50":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d763f0fca48e432ea91b0a348276c5af":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"28e09dcdfa664e419af395c7de153d26":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9P8u6YwQ4oJV","executionInfo":{"status":"ok","timestamp":1628081525858,"user_tz":-480,"elapsed":231047,"user":{"displayName":"韩茏桢","photoUrl":"","userId":"01684388034859812320"}},"outputId":"416fe4b8-a7fe-47b0-aeaa-7582e18238e3"},"source":["# mount google drive folder\n","import os\n","import sys\n","\n","from google.colab import drive\n","\n","drive.mount('/content/gdrive/', force_remount=True)\n","\n","project_dir = '/content/gdrive/My Drive/idgan'\n","\n","sys.path.append(project_dir)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LTqrdzzYtxXI","executionInfo":{"status":"ok","timestamp":1628081603125,"user_tz":-480,"elapsed":499,"user":{"displayName":"韩茏桢","photoUrl":"","userId":"01684388034859812320"}}},"source":["import random\n","from math import sqrt\n","\n","import numpy as np\n","import torch\n","from torch import nn\n","import torch.optim as optim\n","from torch import distributions\n","import torch.nn.functional as F\n","from torchvision import transforms\n","from torchvision.utils import make_grid\n","from torch.utils.tensorboard import SummaryWriter\n","import torchvision.datasets as datasets\n","\n","from tqdm.notebook import tqdm\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-X4PQKZ-rqO-"},"source":["## Preprocess\n","\n","We use the CelebA dataset, resize them to 64, 128 and 256. We will use 64x64 images to train Beta VAE."]},{"cell_type":"code","metadata":{"id":"2XctDx02rNEM","executionInfo":{"status":"ok","timestamp":1628081611789,"user_tz":-480,"elapsed":687,"user":{"displayName":"韩茏桢","photoUrl":"","userId":"01684388034859812320"}}},"source":["from pathlib import Path\n","\n","celeba_64_dir = Path(os.path.join(project_dir, 'CelebA_64'))\n","celeba_128_dir = Path(os.path.join(project_dir, 'CelebA_128'))\n","celeba_256_dir = Path(os.path.join(project_dir, 'CelebA_256'))"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ns4YtfcCtO0w","executionInfo":{"status":"ok","timestamp":1628081613217,"user_tz":-480,"elapsed":3,"user":{"displayName":"韩茏桢","photoUrl":"","userId":"01684388034859812320"}}},"source":["from PIL import Image\n","\n","def preprocess_celeba(path):\n","    crop = transforms.CenterCrop((160, 160))\n","    resample = Image.LANCZOS\n","    img = Image.open(path)\n","    img = crop(img)\n","\n","    img_256_path = celeba_256_dir / path.name \n","    img.resize((256, 256), resample=resample).save(img_256_path)\n","\n","    img_128_path = celeba_128_dir / path.name \n","    img.resize((128, 128), resample=resample).save(img_128_path)\n","\n","    img_64_path = celeba_64_dir / path.name \n","    img.resize((64, 64), resample=resample).save(img_64_path)\n","\n","    return None"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"BgEXgLc8r1AB"},"source":["# from multiprocessing import Pool\n","\n","# paths = list(Path(os.path.join(project_dir, 'img_align_celeba')).glob('*.jpg'))\n","\n","# with Pool(16) as pool:\n","#     pool.map(preprocess_celeba, paths)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vi65cLduvXF5","executionInfo":{"status":"ok","timestamp":1628082032081,"user_tz":-480,"elapsed":417604,"user":{"displayName":"韩茏桢","photoUrl":"","userId":"01684388034859812320"}},"outputId":"e3463da0-d403-4c4e-a979-a7baf19d1c83"},"source":["!ls /content/gdrive/My\\ Drive/idgan/img_align_celeba | wc -l\n","!ls /content/gdrive/My\\ Drive/idgan/CelebA_64/images | wc -l\n","!ls /content/gdrive/My\\ Drive/idgan/CelebA_128/images | wc -l\n","!ls /content/gdrive/My\\ Drive/idgan/CelebA_256/images | wc -l"],"execution_count":7,"outputs":[{"output_type":"stream","text":["ls: cannot open directory '/content/gdrive/My Drive/idgan/img_align_celeba': Input/output error\n","0\n","15031\n","15031\n","15031\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1jZVKqG8r0no"},"source":["## Dataloader\n","\n","PyTorch dataloader"]},{"cell_type":"code","metadata":{"id":"3JBFT8Isr5Dr","executionInfo":{"status":"ok","timestamp":1628081282229,"user_tz":-480,"elapsed":2024,"user":{"displayName":"韩茏桢","photoUrl":"","userId":"01684388034859812320"}}},"source":["from torch.utils.data import Dataset, DataLoader\n","from torchvision.datasets import ImageFolder\n","\n","class CustomImageFolder(ImageFolder):\n","    def __init__(self, root, transform=None):\n","        super(CustomImageFolder, self).__init__(root, transform)\n","\n","    def __getitem__(self, index):\n","        path = self.imgs[index][0]\n","        img = self.loader(path)\n","        if self.transform is not None:\n","            img = self.transform(img)\n","\n","        return img\n","\n","def return_data(image_size = 64):\n","\n","    batch_size = 64\n","    num_workers = 1\n","\n","    root = os.path.join(project_dir, 'CelebA_{}'.format(image_size))\n","    transform = transforms.Compose([\n","        transforms.Resize((image_size, image_size)),\n","        transforms.ToTensor(),\n","        ])\n","    train_kwargs = {'root':root, 'transform':transform}\n","\n","    train_data = CustomImageFolder(**train_kwargs)\n","    train_loader = DataLoader(train_data,\n","                              batch_size=batch_size,\n","                              shuffle=True,\n","                              num_workers=num_workers,\n","                              pin_memory=True,\n","                              drop_last=True)\n","\n","    return train_loader"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pTrRCpyLuOi8","executionInfo":{"status":"ok","timestamp":1628083853530,"user_tz":-480,"elapsed":43417,"user":{"displayName":"韩茏桢","photoUrl":"","userId":"01684388034859812320"}},"outputId":"a43041c6-2037-4a94-c87d-e881894e5120"},"source":["celeba64dataloader = return_data()\n","\n","for idx, img in enumerate(celeba64dataloader):\n","    print(img.shape)\n","    break\n","\n","len(celeba64dataloader) # total batch"],"execution_count":10,"outputs":[{"output_type":"stream","text":["torch.Size([64, 3, 64, 64])\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["234"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"I99ugxgqyKUg"},"source":["## BetaVAE_H\n","\n","Define Beta VAE model"]},{"cell_type":"code","metadata":{"id":"897Yzg-ZzASY","executionInfo":{"status":"ok","timestamp":1628083853531,"user_tz":-480,"elapsed":10,"user":{"displayName":"韩茏桢","photoUrl":"","userId":"01684388034859812320"}}},"source":["def normal_init(m):\n","    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):\n","        m.weight.data.normal_(mean=0, std=0.02)\n","        if m.bias.data is not None:\n","            m.bias.data.zero_()\n","\n","\n","def reparametrize(mu, logvar):\n","    std = logvar.div(2).exp()\n","    eps = std.data.new(std.size()).normal_()\n","    return mu + std*eps\n","\n","\n","class View(nn.Module):\n","    def __init__(self, size):\n","        super(View, self).__init__()\n","        self.size = size\n","    def forward(self, tensor):\n","        return tensor.view(self.size)\n","\n","\n","class Encoder(nn.Module):\n","    def __init__(self, c_dim=10, nc=3, infodistil_mode=False):\n","        super(Encoder, self).__init__()\n","        self.c_dim = c_dim\n","        self.nc = nc\n","        self.infodistil_mode = infodistil_mode\n","        self.layer = nn.Sequential(\n","            nn.Conv2d(nc, 32, 4, 2, 1),          # B,  32, 32, 32\n","            nn.ReLU(True),\n","            nn.Conv2d(32, 32, 4, 2, 1),          # B,  32, 16, 16\n","            nn.ReLU(True),\n","            nn.Conv2d(32, 64, 4, 2, 1),          # B,  64,  8,  8\n","            nn.ReLU(True),\n","            nn.Conv2d(64, 64, 4, 2, 1),          # B,  64,  4,  4\n","            nn.ReLU(True),\n","            nn.Conv2d(64, 256, 4, 1),            # B, 256,  1,  1\n","            nn.ReLU(True),\n","            View((-1, 256*1*1)),                 # B, 256\n","            nn.Linear(256, c_dim*2),             # B, c_dim*2\n","        )\n","\n","    def forward(self, x):\n","        if self.infodistil_mode:\n","            x = x.add(1).div(2)\n","            if (x.size(2) > 64) or (x.size(3) > 64):\n","                x = F.adaptive_avg_pool2d(x, (64, 64))\n","\n","        h = self.layer(x)\n","        return h\n","\n","\n","class Decoder(nn.Module):\n","    def __init__(self, c_dim=10, nc=3):\n","        super(Decoder, self).__init__()\n","        self.c_dim = c_dim\n","        self.nc = nc\n","        self.layer = nn.Sequential(\n","            nn.Linear(c_dim, 256),               # B, 256\n","            View((-1, 256, 1, 1)),               # B, 256,  1,  1\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(256, 64, 4),      # B,  64,  4,  4\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(64, 64, 4, 2, 1), # B,  64,  8,  8\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(64, 32, 4, 2, 1), # B,  32, 16, 16\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(32, 32, 4, 2, 1), # B,  32, 32, 32\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(32, nc, 4, 2, 1),  # B, nc, 64, 64\n","        )\n","\n","\n","    def forward(self, c):\n","        x = self.layer(c)\n","        return x"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"OcyCJSTyyN_K","executionInfo":{"status":"ok","timestamp":1628083853531,"user_tz":-480,"elapsed":6,"user":{"displayName":"韩茏桢","photoUrl":"","userId":"01684388034859812320"}}},"source":["class BetaVAE_H(nn.Module):\n","    \"\"\"Model proposed in original beta-VAE paper(Higgins et al, ICLR, 2017).\"\"\"\n","\n","    def __init__(self, c_dim=10, nc=3, infodistil_mode=False):\n","        super(BetaVAE_H, self).__init__()\n","        self.c_dim = c_dim\n","        self.nc = nc\n","        self.encoder = Encoder(c_dim, nc, infodistil_mode)\n","        self.decoder = Decoder(c_dim, nc)\n","        self.apply(normal_init)\n","\n","    def forward(self, x, c, encode_only, decode_only):\n","        if encode_only:\n","            c, mu, logvar = self._encode(x)\n","            return c, mu, logvar\n","        elif decode_only:\n","            x_recon = self._decode(c)\n","            return x_recon\n","        else:\n","            c, mu, logvar = self._encode(x)\n","            x_recon = self._decode(c)\n","            return x_recon, c, mu, logvar\n","\n","    def __call__(self, x=None, c=None, encode_only=False, decode_only=False):\n","        return self.forward(x, c, encode_only, decode_only)\n","\n","    def _encode(self, x):\n","        distributions = self.encoder(x)\n","        mu = distributions[:, :self.c_dim]\n","        logvar = distributions[:, self.c_dim:]\n","        c = reparametrize(mu, logvar)\n","        return c, mu, logvar\n","\n","    def _decode(self, c):\n","        return self.decoder(c)"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oS2E8XfYuh-1"},"source":["## Solver class\n","\n","VAE training class, the loss id KLD loss and MSE loss"]},{"cell_type":"code","metadata":{"id":"XrrlVNUHhkob","executionInfo":{"status":"ok","timestamp":1628083854161,"user_tz":-480,"elapsed":6,"user":{"displayName":"韩茏桢","photoUrl":"","userId":"01684388034859812320"}}},"source":["def reconstruction_loss(x, x_recon, distribution):\n","    batch_size = x.size(0)\n","    assert batch_size != 0\n","\n","    if distribution == 'bernoulli':\n","        recon_loss = F.binary_cross_entropy_with_logits(x_recon, x, reduction='sum').div(batch_size)\n","    elif distribution == 'gaussian':\n","        x_recon = torch.sigmoid(x_recon)\n","        recon_loss = F.mse_loss(x_recon, x, reduction='sum').div(batch_size)\n","    else:\n","        raise NotImplementedError\n","\n","    return recon_loss\n","\n","\n","def kl_divergence(mu, logvar):\n","    batch_size = mu.size(0)\n","    assert batch_size != 0\n","\n","    if mu.data.ndimension() == 4:\n","        mu = mu.view(mu.size(0), mu.size(1))\n","    if logvar.data.ndimension() == 4:\n","        logvar = logvar.view(logvar.size(0), logvar.size(1))\n","\n","    kld = (-0.5*(1 + logvar - mu.pow(2) - logvar.exp())).sum(1).mean(0, True)\n","    return kld\n","\n","\n","class Solver(object):\n","    def __init__(self, max_iter, data_loader, load_ckpt=None):\n","        \n","        self.global_iter = 0\n","\n","        self.max_iter = max_iter # 1e6 #3e5\n","        self.ckpt_save_iter = 10000\n","        self.log_line_iter = 100\n","        self.log_img_iter = 100\n","        # image output dir\n","        # self.output_dir = os.path.join(project_dir, 'dvae_celeba_output')\n","        # checkpoint save dir\n","        self.ckpt_dir = os.path.join(project_dir, 'ckpt')\n","            \n","        self.log_dir = os.path.join(project_dir, 'tensorboard')\n","        self.writer = SummaryWriter(self.log_dir)\n","\n","        self.data_loader = data_loader\n","\n","        self.nc = 3\n","\n","        self.c_dim = 20\n","        self.beta = 6.4\n","        \n","        self.dec_dist = 'gaussian'\n","\n","        self.net = BetaVAE_H(self.c_dim, self.nc).to(device)\n","        self.net.apply(normal_init)\n","\n","        self.optim = optim.Adam(self.net.parameters(), lr=1e-4,\n","                                betas=(0.9, 0.999))\n","        \n","        if load_ckpt is not None:\n","            self.load_checkpoint(str(load_ckpt))\n","\n","    def train(self):\n","        pbar = tqdm(total=self.max_iter)\n","        pbar.update(self.global_iter)\n","        out = False if self.global_iter < self.max_iter else True\n","        while not out:\n","            for x in self.data_loader:\n","                self.net.train()\n","                self.global_iter += 1\n","                pbar.update(1)\n","\n","                x = x.to(device)\n","                x_recon, c, mu, logvar = self.net(x)\n","\n","                recon_loss = reconstruction_loss(x, x_recon, self.dec_dist)\n","                kld = kl_divergence(mu, logvar)\n","                beta_vae_loss = recon_loss + self.beta*kld\n","\n","                self.optim.zero_grad()\n","                beta_vae_loss.backward()\n","                self.optim.step()\n","\n","                pbar.set_description('[{}] recon_loss:{:.3f} kld:{:.3f}'.format(\n","                    self.global_iter, recon_loss.item(), kld.item()))\n","\n","                if self.global_iter % self.log_line_iter == 0:\n","                    self.writer.add_scalar('recon_loss', recon_loss, self.global_iter)\n","                    self.writer.add_scalar('kld', kld, self.global_iter)\n","\n","                if self.global_iter % self.log_img_iter == 0:\n","                    # visualize reconstruction \n","                    x = make_grid(x, nrow=int(sqrt(x.size(0))), padding=2, pad_value=1)\n","                    x_recon = make_grid(x_recon.sigmoid(), nrow=int(sqrt(x_recon.size(0))), padding=2, pad_value=1)\n","                    x_vis = make_grid(torch.stack([x, x_recon]), nrow=2, padding=2, pad_value=0)\n","                    self.writer.add_image('reconstruction', x_vis, self.global_iter)\n","\n","                    # visualize traverse\n","                    self.traverse(c_post=mu[:1], c_prior=torch.randn_like(mu[:1]))\n","\n","\n","                if self.global_iter % self.ckpt_save_iter == 0:\n","                    self.save_checkpoint()\n","                    pbar.write('Saved checkpoint (iter:{})'.format(self.global_iter))\n","\n","                if self.global_iter >= self.max_iter:\n","                    self.save_checkpoint()\n","                    pbar.write('Saved checkpoint (iter:{})'.format(self.global_iter))\n","                    out = True\n","                    break\n","\n","        pbar.write(\"[Training Finished]\")\n","        pbar.close()\n","\n","    def traverse(self, c_post, c_prior, limit=3, npoints=7, pos=-1):\n","        assert isinstance(pos, (int, list, tuple))\n","\n","        self.net.eval()\n","        c_dict = {'c_posterior':c_post, 'c_prior':c_prior}\n","        interpolation = torch.linspace(-limit, limit, npoints)\n","\n","        for c_key in c_dict:\n","            c_ori = c_dict[c_key]\n","            samples = []\n","            for row in range(self.c_dim):\n","                if pos != -1 and row not in pos:\n","                    continue\n","\n","                c = c_ori.clone()\n","                for val in interpolation:\n","                    c[:, row] = val\n","                    sample = self.net(c=c, decode_only=True).sigmoid().data\n","                    samples.append(sample)\n","\n","            samples = torch.cat(samples, dim=0).cpu()\n","            samples = make_grid(samples, nrow=npoints, padding=2, pad_value=1)\n","            tag = 'latent_traversal_{}'.format(c_key)\n","            self.writer.add_image(tag, samples, self.global_iter)\n","\n","        self.net.train()\n","\n","    def save_checkpoint(self):\n","        model_states = {'net':self.net.state_dict(),\n","                        'c_dim':self.c_dim,\n","                        'nc':self.nc}\n","        optim_states = {'optim':self.optim.state_dict(),}\n","        states = {'iter':self.global_iter,\n","                  'model_states':model_states,\n","                  'optim_states':optim_states}\n","\n","        file_path = os.path.join(self.ckpt_dir, str(self.global_iter))\n","        with open(file_path, mode='wb+') as f:\n","            torch.save(states, f)\n","\n","        # file_path = os.path.join(self.ckpt_dir, 'last')\n","        # with open(file_path, mode='wb+') as f:\n","        #     torch.save(states, f)\n","\n","    def load_checkpoint(self, filename):\n","\n","        file_path = os.path.join(self.ckpt_dir, filename)\n","\n","        if os.path.isfile(file_path):\n","            \n","            checkpoint = torch.load(file_path)\n","            self.global_iter = checkpoint['iter']\n","            self.net.load_state_dict(checkpoint['model_states']['net'])\n","            self.optim.load_state_dict(checkpoint['optim_states']['optim'])\n","\n","            tqdm.write(\"=> loaded checkpoint '{} (iter {})'\".format(file_path, self.global_iter))\n","        else:\n","            tqdm.write(\"=> no checkpoint found at '{}'\".format(file_path))"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QEIVwlcqDYiN"},"source":["## VAE training"]},{"cell_type":"code","metadata":{"id":"EQW_7fdtx7RB"},"source":["\"\"\"\n","那么cuDNN使用的非确定性算法就会自动寻找最适合当前配置的高效算法，来达到优化运行效率的问题\n","\n","一般来讲，应该遵循以下准则：\n","\n","如果网络的输入数据维度或类型上变化不大，设置  torch.backends.cudnn.benchmark = true  可以增加运行效率；\n","如果网络的输入数据在每次 iteration 都变化的话，会导致 cnDNN 每次都会去寻找一遍最优配置，这样反而会降低运行效率。\n","\"\"\"\n","\n","torch.backends.cudnn.enabled =True  # 说明设置为使用使用非确定性算法\n","torch.backends.cudnn.benchmark = True\n","\n","seed = 46\n","\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","random.seed(seed)\n","np.random.seed(seed)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542,"referenced_widgets":["3da2f321e129438aa361fc61e76d6c58","ab19164b0436479aac261098175892f3","c3dad5276dce48edb5a0584a4c828c6c","faefd5b758a24f4da72799a91babfaaf","5fcdf5f39e574e8ea89e700d5fc1a8c2","68113d15a7d44ae297a0e3e448fb4e50","d763f0fca48e432ea91b0a348276c5af","28e09dcdfa664e419af395c7de153d26"]},"id":"WbGUr1sCD2l7","executionInfo":{"status":"ok","timestamp":1628070165422,"user_tz":-480,"elapsed":2177743,"user":{"displayName":"韩茏桢","photoUrl":"","userId":"01684388034859812320"}},"outputId":"184cc2e9-fbe2-4e70-8b8d-f5e4fa4bcae5"},"source":["net = Solver(max_iter=500000, data_loader=celeba64dataloader, load_ckpt=250000)\n","\n","net.train()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["=> loaded checkpoint '/content/gdrive/My Drive/idgan/ckpt/250000 (iter 250000)'\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3da2f321e129438aa361fc61e76d6c58","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=500000.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saved checkpoint (iter:260000)\n","Saved checkpoint (iter:270000)\n","Saved checkpoint (iter:280000)\n","Saved checkpoint (iter:290000)\n","Saved checkpoint (iter:300000)\n","Saved checkpoint (iter:310000)\n","Saved checkpoint (iter:320000)\n","Saved checkpoint (iter:330000)\n","Saved checkpoint (iter:340000)\n","Saved checkpoint (iter:350000)\n","Saved checkpoint (iter:360000)\n","Saved checkpoint (iter:370000)\n","Saved checkpoint (iter:380000)\n","Saved checkpoint (iter:390000)\n","Saved checkpoint (iter:400000)\n","Saved checkpoint (iter:410000)\n","Saved checkpoint (iter:420000)\n","Saved checkpoint (iter:430000)\n","Saved checkpoint (iter:440000)\n","Saved checkpoint (iter:450000)\n","Saved checkpoint (iter:460000)\n","Saved checkpoint (iter:470000)\n","Saved checkpoint (iter:480000)\n","Saved checkpoint (iter:490000)\n","Saved checkpoint (iter:500000)\n","Saved checkpoint (iter:500000)\n","[Training Finished]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nodY-hpbSeTx"},"source":["## Resnet3\n","\n","Define genrator and discriminator"]},{"cell_type":"code","metadata":{"id":"3YayHfqDSd6D","executionInfo":{"status":"ok","timestamp":1628083790564,"user_tz":-480,"elapsed":631,"user":{"displayName":"韩茏桢","photoUrl":"","userId":"01684388034859812320"}}},"source":["class Generator(nn.Module):\n","    def __init__(self, z_dim, size, nfilter=64, nfilter_max=512, **kwargs):\n","        super().__init__()\n","        self.z_dim = z_dim\n","\n","        s0 = self.s0 = 4\n","        nf = self.nf = nfilter\n","        nf_max = self.nf_max = nfilter_max\n","\n","        # Submodules\n","        nlayers = int(np.log2(size / s0))\n","        self.nf0 = min(nf_max, nf * 2**nlayers)\n","\n","        self.fc = nn.Linear(z_dim, self.nf0*s0*s0)\n","\n","        blocks = []\n","        for i in range(nlayers):\n","            nf0 = min(nf * 2**(nlayers-i), nf_max)\n","            nf1 = min(nf * 2**(nlayers-i-1), nf_max)\n","            blocks += [\n","                ResnetBlock(nf0, nf1),\n","                nn.Upsample(scale_factor=2)\n","            ]\n","\n","        blocks += [\n","            ResnetBlock(nf, nf),\n","        ]\n","\n","        self.resnet = nn.Sequential(*blocks)\n","        self.conv_img = nn.Conv2d(nf, 3, 3, padding=1)\n","\n","    def forward(self, z):\n","        batch_size = z.size(0)\n","        out = self.fc(z)\n","        out = out.view(batch_size, self.nf0, self.s0, self.s0)\n","        out = self.resnet(out)\n","        out = self.conv_img(actvn(out))\n","        out = torch.tanh(out)\n","        return out\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, z_dim, size, nfilter=64, nfilter_max=1024):\n","        super().__init__()\n","        s0 = self.s0 = 4\n","        nf = self.nf = nfilter\n","        nf_max = self.nf_max = nfilter_max\n","\n","        # Submodules\n","        nlayers = int(np.log2(size / s0))\n","        self.nf0 = min(nf_max, nf * 2**nlayers)\n","\n","        blocks = [\n","            ResnetBlock(nf, nf)\n","        ]\n","\n","        for i in range(nlayers):\n","            nf0 = min(nf * 2**i, nf_max)\n","            nf1 = min(nf * 2**(i+1), nf_max)\n","            blocks += [\n","                nn.AvgPool2d(3, stride=2, padding=1),\n","                ResnetBlock(nf0, nf1),\n","            ]\n","\n","        self.conv_img = nn.Conv2d(3, 1*nf, 3, padding=1)\n","        self.resnet = nn.Sequential(*blocks)\n","        self.fc = nn.Linear(self.nf0*s0*s0, 1)\n","\n","    def forward(self, x):\n","        batch_size = x.size(0)\n","        out = self.conv_img(x)\n","        out = self.resnet(out)\n","        out = out.view(batch_size, self.nf0*self.s0*self.s0)\n","        out = self.fc(actvn(out))\n","        return out\n","\n","\n","class ResnetBlock(nn.Module):\n","    def __init__(self, fin, fout, fhidden=None, is_bias=True):\n","        super().__init__()\n","        # Attributes\n","        self.is_bias = is_bias\n","        self.learned_shortcut = (fin != fout)\n","        self.fin = fin\n","        self.fout = fout\n","        if fhidden is None:\n","            self.fhidden = min(fin, fout)\n","        else:\n","            self.fhidden = fhidden\n","\n","        # Submodules\n","        self.conv_0 = nn.Conv2d(self.fin, self.fhidden, 3, stride=1, padding=1)\n","        self.conv_1 = nn.Conv2d(self.fhidden, self.fout, 3, stride=1, padding=1, bias=is_bias)\n","        if self.learned_shortcut:\n","            self.conv_s = nn.Conv2d(self.fin, self.fout, 1, stride=1, padding=0, bias=False)\n","\n","    def forward(self, x):\n","        x_s = self._shortcut(x)\n","        dx = self.conv_0(actvn(x))\n","        dx = self.conv_1(actvn(dx))\n","        out = x_s + 0.1*dx\n","\n","        return out\n","\n","    def _shortcut(self, x):\n","        if self.learned_shortcut:\n","            x_s = self.conv_s(x)\n","        else:\n","            x_s = x\n","        return x_s\n","\n","\n","def actvn(x):\n","    out = F.leaky_relu(x, 2e-1)\n","    return out"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NC1GHvzlTTtt"},"source":["## GAN training\n","\n","Start training. To train our GAN model, we need a disentangled representation learned by VAE, we fix the trained encoder and train a Generator for high-fidelity synthesis while distilling the learned disentanglement by optimizing the following objective\n","\n","$\\min_G \\max_D \\mathcal L_{GAN}(D, G) - \\lambda \\mathcal R_{ID}(G)$\n","\n","$\\mathcal L_{GAN} (D, G) = \\mathbb E_{x \\sim p(x)} [\\log D(x)] + \\mathbb E_{s \\sim p(s), c \\sim q_{\\phi}(c) } [\\log (1- D(G(s,c)))]$\n","\n","$\\mathcal R_{ID}(G) = \\mathbb E_{c \\sim q_{\\phi}(c), x \\sim G(s,c)} [\\log q_{\\phi} (c|x)] + H_{q_{\\phi}}(c) $\n","\n","Where $q_{\\phi}(c) = \\frac{1}{N} \\sum_{i} q_{\\phi}(c | x_i)$ is the aggregated posterior of the encoder network."]},{"cell_type":"markdown","metadata":{"id":"WF0sKuEFd4II"},"source":[""]},{"cell_type":"code","metadata":{"id":"iIP82JbeTV8a","executionInfo":{"status":"ok","timestamp":1628083869279,"user_tz":-480,"elapsed":601,"user":{"displayName":"韩茏桢","photoUrl":"","userId":"01684388034859812320"}}},"source":["batch_size = 64\n","d_steps = 1\n","restart_every = -1\n","inception_every = -1\n","save_every = 1000\n","backup_every = 100000\n","\n","# out_dir = os.path.join(project_dir, 'ckpt')\n","checkpoint_dir = os.path.join(project_dir, 'ckpt')\n","\n","c_dim = 20\n","z_dist_dim = 256\n","nc = 3\n","img_size = 64\n","\n","nfilter_generator = 64\n","nfilter_max_generator = 512\n","\n","nfilter_discriminator = 64\n","nfilter_max_discriminator = 512\n","\n","dvae = BetaVAE_H(\n","    c_dim=c_dim,\n","    nc=nc,\n","    infodistil_mode=True\n",")\n","generator = Generator(\n","    z_dim=z_dist_dim + c_dim,\n","    size=img_size,\n","    nfilter=nfilter_generator, \n","    nfilter_max=nfilter_max_generator\n",")\n","discriminator = Discriminator(\n","    z_dim=z_dist_dim + c_dim,\n","    size=img_size,\n","    nfilter=nfilter_discriminator, \n","    nfilter_max=nfilter_max_discriminator\n",")"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1UxJAp9Lufvt","executionInfo":{"status":"ok","timestamp":1628083896303,"user_tz":-480,"elapsed":1189,"user":{"displayName":"韩茏桢","photoUrl":"","userId":"01684388034859812320"}},"outputId":"93805c9d-baaa-4048-b4c8-9b98b3a7f0b9"},"source":["dvae_ckpt = torch.load(os.path.join(project_dir, 'ckpt/500000'))['model_states']['net']\n","dvae.load_state_dict(dvae_ckpt)"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"pfhb7hFbu7Yj","executionInfo":{"status":"ok","timestamp":1628083900007,"user_tz":-480,"elapsed":467,"user":{"displayName":"韩茏桢","photoUrl":"","userId":"01684388034859812320"}}},"source":["dvae = dvae.to(device)\n","generator = generator.to(device)\n","discriminator = discriminator.to(device)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"YBe2hVyjUVxL","executionInfo":{"status":"ok","timestamp":1628083902947,"user_tz":-480,"elapsed":678,"user":{"displayName":"韩茏桢","photoUrl":"","userId":"01684388034859812320"}}},"source":["g_optimizer = optim.RMSprop(generator.parameters(), lr=0.0001, alpha=0.99, eps=1e-8)\n","d_optimizer = optim.RMSprop(discriminator.parameters(), lr=0.0001, alpha=0.99, eps=1e-8)"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"rRz15d6BVG0w","executionInfo":{"status":"ok","timestamp":1628083902948,"user_tz":-480,"elapsed":8,"user":{"displayName":"韩茏桢","photoUrl":"","userId":"01684388034859812320"}}},"source":["def get_zdist(dim, device=None):\n","    # Get distribution\n","    mu = torch.zeros(dim, device=device)\n","    scale = torch.ones(dim, device=device)\n","    zdist = distributions.Normal(mu, scale)\n","\n","    # Add dim attribute\n","    zdist.dim = dim\n","\n","    return zdist\n","\n","cdist = get_zdist(c_dim, device=device)\n","zdist = get_zdist(z_dist_dim, device=device)"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"sFB5CfYfW4tV","executionInfo":{"status":"ok","timestamp":1628083904551,"user_tz":-480,"elapsed":6,"user":{"displayName":"韩茏桢","photoUrl":"","userId":"01684388034859812320"}}},"source":["def build_lr_scheduler(optimizer, last_epoch=-1):\n","    lr_scheduler = optim.lr_scheduler.StepLR(\n","        optimizer,\n","        step_size=150000,\n","        gamma=1,\n","        last_epoch=last_epoch\n","    )\n","    return lr_scheduler\n","\n","# Learning rate anneling\n","g_scheduler = build_lr_scheduler(g_optimizer, last_epoch=-1)\n","d_scheduler = build_lr_scheduler(d_optimizer, last_epoch=-1)"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"3SarCXKTXWlr","executionInfo":{"status":"ok","timestamp":1628083920148,"user_tz":-480,"elapsed":422,"user":{"displayName":"韩茏桢","photoUrl":"","userId":"01684388034859812320"}}},"source":["from torch import autograd\n","\n","class Trainer(object):\n","    def __init__(self, dvae, generator, discriminator, g_optimizer, d_optimizer,\n","                 reg_param, w_info):\n","        self.dvae = dvae\n","        self.generator = generator\n","        self.discriminator = discriminator\n","        self.g_optimizer = g_optimizer\n","        self.d_optimizer = d_optimizer\n","        self.reg_param = reg_param\n","        self.w_info = w_info\n","\n","    def generator_trainstep(self, z, cs):\n","        toogle_grad(self.generator, True)\n","        toogle_grad(self.dvae, True)\n","        toogle_grad(self.discriminator, False)\n","        self.generator.train()\n","        self.discriminator.train()\n","        self.dvae.train()\n","        self.dvae.zero_grad()\n","        self.g_optimizer.zero_grad()\n","\n","        loss = 0.\n","        c, c_mu, c_logvar = cs\n","        z_ = torch.cat([z, c], 1)\n","        x_fake = self.generator(z_)\n","        d_fake = self.discriminator(x_fake)\n","\n","        gloss = self.compute_loss(d_fake, 1)\n","        loss += gloss\n","\n","        chs = self.dvae(x_fake, encode_only=True)\n","        encloss = self.compute_infomax(cs, chs)\n","        loss += self.w_info*encloss\n","\n","        loss.backward()\n","        self.g_optimizer.step()\n","\n","        return gloss.item(), encloss.item()\n","\n","    def discriminator_trainstep(self, x_real, z):\n","        toogle_grad(self.generator, False)\n","        toogle_grad(self.dvae, False)\n","        toogle_grad(self.discriminator, True)\n","        self.generator.train()\n","        self.discriminator.train()\n","        self.dvae.train()\n","        self.d_optimizer.zero_grad()\n","\n","        # On real data\n","        x_real.requires_grad_()\n","\n","        d_real = self.discriminator(x_real)\n","        dloss_real = self.compute_loss(d_real, 1)\n","        dloss_real.backward(retain_graph=True)\n","        reg = self.reg_param * compute_grad2(d_real, x_real).mean()\n","        reg.backward()\n","\n","        # On fake data\n","        with torch.no_grad():\n","            c, c_mu, c_logvar = cs = self.dvae(x_real, encode_only=True)\n","            z_ = torch.cat([z, c], 1)\n","            x_fake = self.generator(z_)\n","\n","        x_fake.requires_grad_()\n","        d_fake = self.discriminator(x_fake)\n","        dloss_fake = self.compute_loss(d_fake, 0)\n","        dloss_fake.backward()\n","\n","        self.d_optimizer.step()\n","        toogle_grad(self.discriminator, False)\n","\n","        # Output\n","        dloss = (dloss_real + dloss_fake)\n","\n","        return dloss.item(), reg.item(), cs\n","\n","    def compute_loss(self, d_out, target):\n","        targets = d_out.new_full(size=d_out.size(), fill_value=target)\n","        loss = F.binary_cross_entropy_with_logits(d_out, targets)\n","        return loss\n","\n","    def compute_infomax(self, cs, chs):\n","        c, c_mu, c_logvar = cs\n","        ch, ch_mu, ch_logvar = chs\n","        loss = (math.log(2*math.pi) + ch_logvar + (c-ch_mu).pow(2).div(ch_logvar.exp()+1e-8)).div(2).sum(1).mean()\n","        return loss\n","\n","\n","# Utility functions\n","def toogle_grad(model, requires_grad):\n","    for p in model.parameters():\n","        p.requires_grad_(requires_grad)\n","\n","\n","def compute_grad2(d_out, x_in):\n","    batch_size = x_in.size(0)\n","    grad_dout = autograd.grad(\n","        outputs=d_out.sum(), inputs=x_in,\n","        create_graph=True, retain_graph=True, only_inputs=True\n","    )[0]\n","    grad_dout2 = grad_dout.pow(2)\n","    assert(grad_dout2.size() == x_in.size())\n","    reg = grad_dout2.view(batch_size, -1).sum(1)\n","    return reg\n","\n","\n","def update_average(model_tgt, model_src, beta):\n","    toogle_grad(model_src, False)\n","    toogle_grad(model_tgt, False)\n","\n","    param_dict_src = dict(model_src.named_parameters())\n","\n","    for p_name, p_tgt in model_tgt.named_parameters():\n","        p_src = param_dict_src[p_name]\n","        assert(p_src is not p_tgt)\n","        p_tgt.copy_(beta*p_tgt + (1. - beta)*p_src)"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"nIJ7AjenjeeB","executionInfo":{"status":"ok","timestamp":1628083927860,"user_tz":-480,"elapsed":429,"user":{"displayName":"韩茏桢","photoUrl":"","userId":"01684388034859812320"}}},"source":["class CheckpointIO(object):\n","    def __init__(self, checkpoint_dir='./chkpts', **kwargs):\n","        self.module_dict = kwargs\n","        self.checkpoint_dir = checkpoint_dir\n","\n","        if not os.path.exists(checkpoint_dir):\n","            os.makedirs(checkpoint_dir)\n","\n","    def register_modules(self, **kwargs):\n","        self.module_dict.update(kwargs)\n","\n","    def save(self, it, filename):\n","        filename = os.path.join(self.checkpoint_dir, filename)\n","\n","        outdict = {'it': it}\n","        for k, v in self.module_dict.items():\n","            outdict[k] = v.state_dict()\n","        torch.save(outdict, filename)\n","\n","    def load(self, filename):\n","        filename = os.path.join(self.checkpoint_dir, filename)\n","\n","        if os.path.exists(filename):\n","            tqdm.write('=> Loading checkpoint...')\n","            out_dict = torch.load(filename)\n","            it = out_dict['it']\n","            for k, v in self.module_dict.items():\n","                if k in out_dict:\n","                    v.load_state_dict(out_dict[k])\n","                else:\n","                    tqdm.write('Warning: Could not find %s in checkpoint!' % k)\n","        else:\n","            it = -1\n","\n","        return it\n","\n","checkpoint_io = CheckpointIO(\n","    checkpoint_dir=os.path.join(project_dir, 'ckpt')\n",")\n","\n","# Register modules to checkpoint\n","checkpoint_io.register_modules(\n","    generator=generator,\n","    discriminator=discriminator,\n","    g_optimizer=g_optimizer,\n","    d_optimizer=d_optimizer,\n",")"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2yr_i9JkmkmN","executionInfo":{"status":"ok","timestamp":1628083938191,"user_tz":-480,"elapsed":501,"user":{"displayName":"韩茏桢","photoUrl":"","userId":"01684388034859812320"}},"outputId":"661180b3-fc47-4c4d-ebd1-24f313609f40"},"source":["import torchvision.datasets as datasets\n","\n","transform = transforms.Compose([\n","    transforms.Resize(img_size),\n","    transforms.CenterCrop(img_size),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","    transforms.Lambda(lambda x: x + 1./128 * torch.rand(x.size())),\n","])\n","\n","train_dataset = datasets.ImageFolder(celeba_256_dir, transform)\n","\n","train_loader = torch.utils.data.DataLoader(\n","        train_dataset,\n","        batch_size=batch_size,\n","        num_workers=16,\n","        shuffle=True, pin_memory=True, sampler=None, drop_last=True\n",")"],"execution_count":22,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"ELLWqtU8m3v9","executionInfo":{"status":"ok","timestamp":1628083943248,"user_tz":-480,"elapsed":461,"user":{"displayName":"韩茏桢","photoUrl":"","userId":"01684388034859812320"}}},"source":["trainer = Trainer(\n","    dvae, generator, discriminator, g_optimizer, d_optimizer,\n","    reg_param=10,\n","    w_info = 0.001\n",")"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"prYj2-jZXuD-","executionInfo":{"status":"ok","timestamp":1628096994466,"user_tz":-480,"elapsed":5294625,"user":{"displayName":"韩茏桢","photoUrl":"","userId":"01684388034859812320"}},"outputId":"2aba6da6-9111-403a-a8fe-5cd122f950ab"},"source":["import math\n","import time\n","\n","max_iter = 60000 # 300000\n","pbar = tqdm(total=max_iter)\n","it = -1\n","epoch_idx = -1\n","tstart = t0 = time.time()\n","\n","# it = epoch_idx = checkpoint_io.load(os.path.join(checkpoint_dir, 'model_00030000.pt'))\n","\n","out = False\n","while not out:\n","    epoch_idx += 1\n","    tqdm.write('Start epoch %d...' % epoch_idx)\n","\n","    for x_real, _ in train_loader:\n","        it += 1\n","        pbar.update(1)\n","        g_scheduler.step()\n","        d_scheduler.step()\n","\n","        d_lr = d_optimizer.param_groups[0]['lr']\n","        g_lr = g_optimizer.param_groups[0]['lr']\n","\n","        x_real = x_real.to(device)\n","\n","        # Discriminator updates\n","        z = zdist.sample((batch_size,))\n","        dloss, reg, cs = trainer.discriminator_trainstep(x_real, z)\n","\n","        # Generators updates\n","        if ((it + 1) % d_steps) == 0:\n","            z = zdist.sample((batch_size,))\n","            gloss, encloss = trainer.generator_trainstep(z, cs)\n","\n","        # (iii) Backup if necessary\n","        if ((it + 1) % backup_every) == 0:\n","            tqdm.write('Saving backup...')\n","            checkpoint_io.save(it, 'model_%08d.pt' % it)\n","            checkpoint_io.save(it, 'model.pt')\n","\n","        # (iv) Save checkpoint if necessary\n","        if time.time() - t0 > save_every:\n","            tqdm.write('Saving checkpoint...')\n","            checkpoint_io.save(it, 'model.pt')\n","            t0 = time.time()\n","\n","        if it >= max_iter:\n","            tqdm.write('Saving backup...')\n","            checkpoint_io.save(it, 'model_%08d.pt' % it)\n","            # logger.save_stats('stats_%08d.p' % it)\n","            checkpoint_io.save(it, 'model.pt')\n","            # logger.save_stats('stats.p')\n","            out = True\n","            break"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Start epoch 149...\n","Start epoch 150...\n","Start epoch 151...\n","Start epoch 152...\n","Start epoch 153...\n","Start epoch 154...\n","Saving checkpoint...\n","Start epoch 155...\n","Start epoch 156...\n","Start epoch 157...\n","Start epoch 158...\n","Start epoch 159...\n","Start epoch 160...\n","Start epoch 161...\n","Start epoch 162...\n","Start epoch 163...\n","Start epoch 164...\n","Start epoch 165...\n","Start epoch 166...\n","Start epoch 167...\n","Start epoch 168...\n","Start epoch 169...\n","Start epoch 170...\n","Start epoch 171...\n","Start epoch 172...\n","Start epoch 173...\n","Start epoch 174...\n","Saving checkpoint...\n","Start epoch 175...\n","Start epoch 176...\n","Start epoch 177...\n","Start epoch 178...\n","Start epoch 179...\n","Start epoch 180...\n","Start epoch 181...\n","Start epoch 182...\n","Start epoch 183...\n","Start epoch 184...\n","Start epoch 185...\n","Start epoch 186...\n","Start epoch 187...\n","Start epoch 188...\n","Start epoch 189...\n","Start epoch 190...\n","Start epoch 191...\n","Start epoch 192...\n","Start epoch 193...\n","Start epoch 194...\n","Start epoch 195...\n","Saving checkpoint...\n","Start epoch 196...\n","Start epoch 197...\n","Start epoch 198...\n","Start epoch 199...\n","Start epoch 200...\n","Start epoch 201...\n","Start epoch 202...\n","Start epoch 203...\n","Start epoch 204...\n","Start epoch 205...\n","Start epoch 206...\n","Start epoch 207...\n","Start epoch 208...\n","Start epoch 209...\n","Start epoch 210...\n","Start epoch 211...\n","Start epoch 212...\n","Start epoch 213...\n","Start epoch 214...\n","Start epoch 215...\n","Saving checkpoint...\n","Start epoch 216...\n","Start epoch 217...\n","Start epoch 218...\n","Start epoch 219...\n","Start epoch 220...\n","Start epoch 221...\n","Start epoch 222...\n","Start epoch 223...\n","Start epoch 224...\n","Start epoch 225...\n","Start epoch 226...\n","Start epoch 227...\n","Start epoch 228...\n","Start epoch 229...\n","Start epoch 230...\n","Start epoch 231...\n","Start epoch 232...\n","Start epoch 233...\n","Start epoch 234...\n","Start epoch 235...\n","Saving checkpoint...\n","Start epoch 236...\n","Start epoch 237...\n","Start epoch 238...\n","Start epoch 239...\n","Start epoch 240...\n","Start epoch 241...\n","Start epoch 242...\n","Start epoch 243...\n","Start epoch 244...\n","Start epoch 245...\n","Start epoch 246...\n","Start epoch 247...\n","Start epoch 248...\n","Start epoch 249...\n","Start epoch 250...\n","Start epoch 251...\n","Start epoch 252...\n","Start epoch 253...\n","Start epoch 254...\n","Start epoch 255...\n","Start epoch 256...\n","Saving checkpoint...\n","Saving backup...\n"],"name":"stdout"}]}]}